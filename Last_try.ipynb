{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99d7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textblob as tb\n",
    "randstate=np.random.randint(0,1000)\n",
    "\n",
    "\n",
    "def get_text_features(df):\n",
    "    \"\"\"\n",
    "    Extracts deterministic NLP features from the 'Opinion' column.\n",
    "    \"\"\"\n",
    "    # 1. Clean Text\n",
    "    # fillna(\"\") ensures we can run string methods even on missing data\n",
    "    clean_op = (\n",
    "        df[\"Opinion\"]\n",
    "        .fillna(\"\")\n",
    "        .str.lower()\n",
    "        .str.replace(r\"[^a-z\\s!?.]\", \"\", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "    \n",
    "    # 2. Structural Features\n",
    "    df[\"op_word_count\"] = clean_op.str.split().apply(len)\n",
    "    \n",
    "    # 3. Intensity Features\n",
    "    df[\"exclam_count\"] = clean_op.str.count(\"!\")\n",
    "    df[\"qmark_count\"] = clean_op.str.count(r\"\\?\")\n",
    "    \n",
    "    # 4. Sentiment Features (TextBlob)\n",
    "    # We use a simple apply; for huge datasets, consider swifter or pandarallel\n",
    "    df[\"tb_polarity\"] = clean_op.apply(\n",
    "        lambda x: tb.TextBlob(x).sentiment.polarity if x else 0\n",
    "    )\n",
    "    \n",
    "    # 5. Key Counts (Positive/Negative/Specific words)\n",
    "    negations = {\"not\", \"never\", \"no\", \"nothing\", \"none\"}\n",
    "    df[\"negation_count\"] = clean_op.apply(lambda x: sum(w in negations for w in x.split()))\n",
    "\n",
    "    negative_words = {\"worst\", \"disaster\", \"refund\", \"ruined\", \"terrible\"}\n",
    "    df[\"neg_word_count\"] = clean_op.apply(lambda x: sum(w in negative_words for w in x.split()))\n",
    "\n",
    "    # Specific Keyword Flags (High correlation anchors)\n",
    "    keywords = [\"refund\", \"worst\", \"magic\", \"loved\"]\n",
    "    for kw in keywords:\n",
    "        df[f\"kw_{kw}\"] = clean_op.str.contains(rf\"\\b{kw}\\b\", regex=True).astype(int)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def split_ticket(ticket):\n",
    "    try:\n",
    "        parts = str(ticket).split('/')\n",
    "        return parts[0], parts[-1] # Returns (TicketType, TicketClass)\n",
    "    except:\n",
    "        return 'Unknown', 'Unknown'\n",
    "\n",
    "def preprocess_csv_improved(path, save=False, filename=\"\"):\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    # --- 1. NLP & Extraction (Keep your good logic) ---\n",
    "    # (Assuming get_text_features and split_ticket are defined as before)\n",
    "    data = get_text_features(data)\n",
    "    \n",
    "    data[['Ticket_Type', 'Ticket_Class']] = data['TicketInfo'].apply(lambda x: pd.Series(split_ticket(x)))\n",
    "\n",
    "    # --- 2. Handling Missing Values Preserving Information ---\n",
    "    # CATEGORICAL: Fill with \"Missing\" so the model sees it as a specific category\n",
    "    cat_cols = ['Concert', 'Ticket_Type', 'Ticket_Class', 'PreferedAlbum', 'Vinyl', 'VIP']\n",
    "    for col in cat_cols:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].fillna(\"Missing\")\n",
    "\n",
    "    # NUMERICAL: Do NOT impute Median. Leave as NaN.\n",
    "    # HistGradientBoosting handles NaNs automatically.\n",
    "    \n",
    "    # --- 3. Encoding ---\n",
    "    # We drop raw text columns\n",
    "    cols_to_drop = [ 'Opinion', 'TicketInfo']\n",
    "    data = data.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # One-Hot Encoding for categories\n",
    "    df_encoded = pd.get_dummies(data, columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    if save:\n",
    "        df_encoded.to_csv(filename, index=False)\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bf9f432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC: 0.8988 +/- 0.0085\n",
      "Best Threshold on Training: 0.46000000000000013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import os\n",
    "path=os.getcwd()\n",
    "# 1. Load Data\n",
    "# Note: We do NOT need to pass 'stats' anymore because we aren't calculating Medians\n",
    "train_df = preprocess_csv_improved(path+'/train.csv')\n",
    "test_df = preprocess_csv_improved(path+'/test.csv')\n",
    "\n",
    "# 2. Align Columns (Critical Fix)\n",
    "test_df = test_df.reindex(columns=train_df.columns, fill_value=0)\n",
    "\n",
    "# 3. Prepare X/Y\n",
    "# Drop artifacts and targets\n",
    "drop_cols = [\"Id\", \"FreePass\", \"AvgTime\", \"Opinion_raw\", \"Opinion_clean\"] \n",
    "X = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "Y = train_df[\"FreePass\"]\n",
    "\n",
    "X_test_submission = test_df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# 4. Model: HistGradientBoosting\n",
    "# Native support for missing values (nan_mode=\"fraction\" or \"most_frequent\")\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=300,        # Equivalent to n_estimators\n",
    "    max_depth=5,         # Constrain depth\n",
    "    l2_regularization=1.0, # Helps preventing overfitting\n",
    "    random_state=randstate,\n",
    "    early_stopping=True  # Automatically stops if validation score stops improving\n",
    ")\n",
    "\n",
    "# 5. Robust Evaluation (Cross-Validation)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=randstate)\n",
    "scores = cross_val_score(hgb, X, Y, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(f\"CV AUC: {scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "\n",
    "# 6. Final Training & Prediction\n",
    "hgb.fit(X, Y)\n",
    "\n",
    "# Predict Probabilities\n",
    "final_probs = hgb.predict_proba(X_test_submission)[:, 1]\n",
    "\n",
    "# Optimize Threshold (Quick check on Training data - bias warning, but useful for hard cuts)\n",
    "# Ideally, you do this inside a CV loop, but this is a good approximation\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_probs = hgb.predict_proba(X)[:, 1]\n",
    "best_tr = 0.5\n",
    "best_acc = 0\n",
    "for tr in np.arange(0.3, 0.7, 0.01):\n",
    "    acc = accuracy_score(Y, train_probs >= tr)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_tr = tr\n",
    "\n",
    "print(f\"Best Threshold on Training: {best_tr}\")\n",
    "\n",
    "# Export\n",
    "final_preds = (final_probs >= best_tr)\n",
    "output = pd.DataFrame({'Id': test_df['Id'], 'FreePass': final_preds})\n",
    "output.to_csv(path + \"/results/submission_hist_gradient_boost_JAN12.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70b9d99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC: 0.8974 +/- 0.0082\n",
      "Best Threshold on Training: 0.48000000000000015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# 1. Load Data\n",
    "# Note: We do NOT need to pass 'stats' anymore because we aren't calculating Medians\n",
    "train_df = preprocess_csv_improved(path+'/train.csv')\n",
    "test_df = preprocess_csv_improved(path+'/test.csv')\n",
    "\n",
    "# 2. Align Columns (Critical Fix)\n",
    "test_df = test_df.reindex(columns=train_df.columns, fill_value=0)\n",
    "\n",
    "# 3. Prepare X/Y\n",
    "# Drop artifacts and targets\n",
    "drop_cols = [\"Id\", \"FreePass\", \"AvgTime\", \"Opinion_raw\", \"Opinion_clean\"] \n",
    "X = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "Y = train_df[\"FreePass\"]\n",
    "\n",
    "X_test_submission = test_df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# 4. Model: HistGradientBoosting\n",
    "# Native support for missing values (nan_mode=\"fraction\" or \"most_frequent\")\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=100,        # Equivalent to n_estimators\n",
    "    max_depth=5,         # Constrain depth\n",
    "    l2_regularization=1.0, # Helps preventing overfitting\n",
    "    random_state=randstate,\n",
    "    early_stopping=True  # Automatically stops if validation score stops improving\n",
    ")\n",
    "\n",
    "# 5. Robust Evaluation (Cross-Validation)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=randstate)\n",
    "scores = cross_val_score(hgb, X, Y, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(f\"CV AUC: {scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "\n",
    "# 6. Final Training & Prediction\n",
    "hgb.fit(X, Y)\n",
    "\n",
    "# Predict Probabilities\n",
    "final_probs = hgb.predict_proba(X_test_submission)[:, 1]\n",
    "\n",
    "# Optimize Threshold (Quick check on Training data - bias warning, but useful for hard cuts)\n",
    "# Ideally, you do this inside a CV loop, but this is a good approximation\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_probs = hgb.predict_proba(X)[:, 1]\n",
    "best_tr = 0.5\n",
    "best_acc = 0\n",
    "for tr in np.arange(0.3, 0.7, 0.01):\n",
    "    acc = accuracy_score(Y, train_probs >= tr)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_tr = tr\n",
    "\n",
    "print(f\"Best Threshold on Training: {best_tr}\")\n",
    "\n",
    "# Export\n",
    "final_preds = (final_probs >= best_tr)\n",
    "output = pd.DataFrame({'Id': test_df['Id'], 'FreePass': final_preds})\n",
    "output.to_csv(path + \"/results/submission_hist_gradient_boost_JAN12.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b32ecb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble CV AUC: 1.0000 (+/- 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.5800000000000003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer  # <--- NEW IMPORT\n",
    "\n",
    "# 1. Define Model 1: Linear Model (Now with Imputation)\n",
    "clf1 = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),  # <--- CRITICAL FIX: Fills NaNs for this model only\n",
    "    StandardScaler(), \n",
    "    LogisticRegression(\n",
    "        penalty='l1', \n",
    "        solver='liblinear', \n",
    "        C=0.1,\n",
    "        class_weight='balanced',\n",
    "        random_state=randstate\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Define Model 2: Tree Model (Handles NaNs natively, no changes needed)\n",
    "clf2 = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=400,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=20,\n",
    "    l2_regularization=5.0,\n",
    "    random_state=randstate\n",
    ")\n",
    "\n",
    "# 3. The Ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('gb', clf2)],\n",
    "    voting='soft', \n",
    "    weights=[1, 2] \n",
    ")\n",
    "\n",
    "# 4. Train & Evaluate\n",
    "cv_scores = cross_val_score(ensemble, X, Y, cv=5, scoring='roc_auc')\n",
    "print(f\"Ensemble CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# 5. Final Fit & Optimization\n",
    "ensemble.fit(X, Y)\n",
    "\n",
    "# Optimize Threshold\n",
    "train_probs = ensemble.predict_proba(X)[:, 1]\n",
    "best_tr = 0.5\n",
    "best_acc = 0\n",
    "for tr in np.arange(0.3, 0.7, 0.01):\n",
    "    acc = accuracy_score(Y, train_probs >= tr)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_tr = tr\n",
    "\n",
    "print(f\"Best Threshold: {best_tr}\")\n",
    "\n",
    "# 6. Predict & Export\n",
    "final_probs = ensemble.predict_proba(X_test_submission)[:, 1]\n",
    "final_preds = (final_probs >= best_tr)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_df['Id'], 'FreePass': final_preds})\n",
    "output.to_csv(path + \"/results/submission_ensemble_fixed_JAN12.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbcabb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv_improved(path, save=False, filename=\"\"):\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    # --- 1. NLP & Extraction ---\n",
    "    data = get_text_features(data)\n",
    "    \n",
    "    data['Concert_StartTime'] = data['Concert'].astype(str).str.extract(r'([A-Za-z]+)(\\d+)-', expand=False)[1].astype(float) \n",
    "    data['Concert_EndTime'] = data['Concert'].astype(str).str.extract(r'-(\\d+)pm').astype(float)\n",
    "    data['Concert_Duration'] = 12 + data['Concert_EndTime'] - data['Concert_StartTime']\n",
    "\n",
    "    data[['Ticket_Type', 'Ticket_Class']] = data['TicketInfo'].apply(lambda x: pd.Series(split_ticket(x)))\n",
    "\n",
    "    # --- 2. Handling Missing Values ---\n",
    "    cat_cols = ['Ticket_Type', 'Ticket_Class', 'PreferedAlbum', 'Vinyl', 'VIP']\n",
    "    for col in cat_cols:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].fillna(\"Missing\")\n",
    "\n",
    "    # --- 3. Encoding ---\n",
    "    # CRITICAL CHANGE: We do NOT drop 'Concert' here anymore. \n",
    "    # We only drop Opinion and TicketInfo.\n",
    "    cols_to_drop = ['Opinion', 'TicketInfo'] \n",
    "    data = data.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # One-Hot Encoding (Concert is NOT in cat_cols, so it stays as a string column)\n",
    "    df_encoded = pd.get_dummies(data, columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e85facdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 37\u001b[0m\n\u001b[0;32m     30\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m VotingClassifier(\n\u001b[0;32m     31\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, clf1), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgb\u001b[39m\u001b[38;5;124m'\u001b[39m, clf2)],\n\u001b[0;32m     32\u001b[0m     voting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     33\u001b[0m     weights\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m] \n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Run Group Cross-Validation\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m cross_val_score(ensemble, X, Y, groups\u001b[38;5;241m=\u001b[39mgroups, cv\u001b[38;5;241m=\u001b[39mgkf, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRealistic Group-CV AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (+/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:651\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    649\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 651\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    652\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    653\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    654\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    655\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    656\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    657\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    658\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    659\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    660\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    661\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    662\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    663\u001b[0m )\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:373\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 373\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    374\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    375\u001b[0m         clone(estimator),\n\u001b[0;32m    376\u001b[0m         X,\n\u001b[0;32m    377\u001b[0m         y,\n\u001b[0;32m    378\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    379\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    380\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    381\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    382\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m    384\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[0;32m    385\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    386\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    387\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    388\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    389\u001b[0m     )\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    391\u001b[0m )\n\u001b[0;32m    393\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\utils\\parallel.py:91\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     79\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     80\u001b[0m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     81\u001b[0m )\n\u001b[0;32m     83\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     84\u001b[0m     (\n\u001b[0;32m     85\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     90\u001b[0m )\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\joblib\\parallel.py:1844\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;66;03m# Sequentially call the tasks and yield the results.\u001b[39;00m\n\u001b[1;32m-> 1844\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1845\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\utils\\parallel.py:83\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     78\u001b[0m filters_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(warnings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_get_filters\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     79\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     80\u001b[0m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 83\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     84\u001b[0m     (\n\u001b[0;32m     85\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[0;32m     86\u001b[0m         args,\n\u001b[0;32m     87\u001b[0m         kwargs,\n\u001b[0;32m     88\u001b[0m     )\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:373\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 373\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    374\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    375\u001b[0m         clone(estimator),\n\u001b[0;32m    376\u001b[0m         X,\n\u001b[0;32m    377\u001b[0m         y,\n\u001b[0;32m    378\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    379\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    380\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    381\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    382\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m    384\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[0;32m    385\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    386\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    387\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    388\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    389\u001b[0m     )\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    391\u001b[0m )\n\u001b[0;32m    393\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:412\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    406\u001b[0m         (\n\u001b[0;32m    407\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    410\u001b[0m     )\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:143\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    141\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m    142\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(_num_samples(X))\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_test_masks(X, y, groups):\n\u001b[0;32m    144\u001b[0m     train_index \u001b[38;5;241m=\u001b[39m indices[np\u001b[38;5;241m.\u001b[39mlogical_not(test_index)]\n\u001b[0;32m    145\u001b[0m     test_index \u001b[38;5;241m=\u001b[39m indices[test_index]\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:155\u001b[0m, in \u001b[0;36mBaseCrossValidator._iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_test_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates boolean masks corresponding to test sets.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m    By default, delegates to _iter_test_indices(X, y, groups)\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m test_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_test_indices(X, y, groups):\n\u001b[0;32m    156\u001b[0m         test_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(_num_samples(X), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    157\u001b[0m         test_mask[test_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:615\u001b[0m, in \u001b[0;36mGroupKFold._iter_test_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter should not be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 615\u001b[0m groups \u001b[38;5;241m=\u001b[39m check_array(groups, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m\"\u001b[39m, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    617\u001b[0m unique_groups, group_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(groups, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    618\u001b[0m n_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unique_groups)\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\utils\\validation.py:1074\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1071\u001b[0m     )\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1074\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1075\u001b[0m         array,\n\u001b[0;32m   1076\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1077\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1078\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1079\u001b[0m     )\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ale\\miniconda3\\envs\\snakes\\Lib\\site-packages\\sklearn\\utils\\validation.py:118\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_api \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 118\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(X\u001b[38;5;241m.\u001b[39mdtype, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex floating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "# --- PROCESS DATA ---\n",
    "train_df = preprocess_csv_improved(path+'/train.csv')\n",
    "test_df = preprocess_csv_improved(path+'/test.csv')\n",
    "\n",
    "# --- APPLY FEATURE ENGINEERING ---\n",
    "# Now this works because 'Concert' is still in the dataframe\n",
    "train_df = get_advanced_features(train_df)\n",
    "test_df = get_advanced_features(test_df)\n",
    "\n",
    "# Align columns (Test might miss some dummy columns)\n",
    "test_df = test_df.reindex(columns=train_df.columns, fill_value=0)\n",
    "\n",
    "# --- SETUP GROUPS & X/Y ---\n",
    "\n",
    "# 1. Define Groups using the 'Concert' column\n",
    "groups = train_df['Concert'] \n",
    "\n",
    "# 2. Define Drop List\n",
    "# We NOW drop 'Concert' here, right before training\n",
    "# We also drop 'AvgTime' to test the Realistic Score (No Leak)\n",
    "drop_cols = [\"Id\", \"FreePass\", \"Opinion_raw\", \"Opinion_clean\", \"Opinion\", \"Concert\", \"AvgTime\"] \n",
    "\n",
    "X = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "Y = train_df[\"FreePass\"]\n",
    "\n",
    "# --- ROBUST VALIDATION ---\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Re-initialize your ensemble (make sure it's defined)\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('gb', clf2)],\n",
    "    voting='soft', \n",
    "    weights=[1, 2] \n",
    ")\n",
    "\n",
    "# Run Group Cross-Validation\n",
    "cv_scores = cross_val_score(ensemble, X, Y, groups=groups, cv=gkf, scoring='roc_auc')\n",
    "\n",
    "print(f\"Realistic Group-CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
