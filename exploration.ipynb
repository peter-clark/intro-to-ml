{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1476ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c258181",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "raw_data = pd.read_csv(path+\"/train.csv\")\n",
    "raw_data[\"Vinyl\"].value_counts(dropna=False)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe757b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "raw_data_test = pd.read_csv(path+\"/test.csv\")\n",
    "raw_data_test[\"Vinyl\"].value_counts(dropna=False)\n",
    "raw_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b41991",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(raw_data, x=\"AvgTime\")\n",
    "sns.histplot(raw_data_test, x=\"AvgTime\", color=\"orange\")\n",
    "\n",
    "f=sns.histplot(raw_data, x=\"Food\")\n",
    "f.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sns.histplot(raw_data, x=\"Age\")\n",
    "b=sns.histplot(raw_data_test, x=\"Age\", color='orange')\n",
    "\n",
    "d=sns.histplot(raw_data, x=\"Drinks\")\n",
    "e=sns.histplot(raw_data_test, x='Drinks', color='orange')\n",
    "d.set_xscale('log')\n",
    "d.set_yscale('log')\n",
    "\n",
    "album_pass = sns.countplot(data=raw_data,x=\"PreferedAlbum\",hue=\"FreePass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data2 = raw_data[['Food','Drinks',\"FreePass\",\"AvgTime\",\"Vinyl\"]].fillna(value=0.0)\n",
    "col = raw_data['Age'].fillna(value=raw_data['Age'].mean())\n",
    "raw_data2['Age']=col\n",
    "raw_data2['logDrinks']=np.log(raw_data2['Drinks'])\n",
    "raw_data2['logFood']=np.log(raw_data2['Food'])\n",
    "food_pass = sns.histplot(data=raw_data2, x=\"Food\", hue=\"FreePass\", kde=True, element='step', stat='count')\n",
    "food_pass.set_xscale('log')\n",
    "sns.histplot(raw_data2, x=\"Vinyl\", hue=\"FreePass\")\n",
    "drink_pass = sns.histplot(data=raw_data2, x=\"Drinks\", hue=\"FreePass\", kde=True, element='step', stat='count')\n",
    "drink_pass.set_xscale('log')\n",
    "age_pass = sns.histplot(data=raw_data,x=\"Age\", hue=\"FreePass\",kde=True, element='step',stat='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18af165",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix=pd.DataFrame.corr(raw_data2)\n",
    "sns.heatmap(corr_matrix, vmin=-1,vmax=1,cmap='viridis',)\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jntplt2 = sns.histplot(raw_data2, x='Age', y='logDrinks', hue='FreePass', multiple='layer')\n",
    "jntplt2.set_xlim(0,100)\n",
    "\n",
    "jnt_fooddrink = sns.JointGrid(data=raw_data, x=raw_data['PreferedAlbum'].astype('category'), y='Food', hue='FreePass')\n",
    "jnt_fooddrink.plot(sns.histplot, sns.histplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import textblob as tb\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c601cc",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "## ML Models and Predicting FreePass ##\n",
    "\n",
    "Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714007f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, log_loss\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ticket(ticket):\n",
    "        try:\n",
    "            parts = str(ticket).split('/')\n",
    "            # Returns (TicketType, TicketClass)\n",
    "            # Example: 'CB/734/XL' -> 'CB', 'XL'\n",
    "            return parts[0], int(parts[1]), parts[-1]\n",
    "        except:\n",
    "            return None, 0, None\n",
    "\n",
    "sia=SentimentIntensityAnalyzer()\n",
    "def vader_polarity(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return 0.0\n",
    "    return sia.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "\n",
    "def preprocess_csv(path, save=False, filename=\"\"):\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    # Fill NaNs #\n",
    "    # Numerical Columns with Median #\n",
    "    print(f\"Missing values: {data.isnull().sum().sum()}\")\n",
    "    print(f\"Missing values in Age: {data['Age'].isnull().sum()}\") \n",
    "    data['Age'] = data['Age'].fillna(data['Age'].median())\n",
    "    # Categorical Columns with Mode#\n",
    "    print(f\"Missing values in Concert: {data['Concert'].isnull().sum()}\")\n",
    "    print(f\"Missing values in TicketInfo: {data['TicketInfo'].isnull().sum()}\")\n",
    "    print(f\"Missing values in PreferedAlbum: {data['PreferedAlbum'].isnull().sum()}\")\n",
    "    print(f\"Missing values in Vinyl: {data['Vinyl'].isnull().sum()}\")\n",
    "    print(f\"Missing values in VIP: {data['VIP'].isnull().sum()}\")\n",
    "    cat_cols = ['Concert', 'TicketInfo', 'PreferedAlbum', 'Vinyl', 'VIP']\n",
    "    for col in cat_cols:\n",
    "        # Mode returns a series, we take the first element [0]\n",
    "        data[col] = data[col].fillna(data[col].mode()[0])\n",
    "    print(f\"Still missing values: {data.isnull().sum().sum()}\")\n",
    "\n",
    "    data[\"Opinion_Polarity\"] = (data[\"Opinion\"].apply(preprocess_text).apply(vader_polarity))\n",
    "    data['Concert_City'] = data['Concert'].astype(str).str.extract(r'^([A-Za-z]+)') # concert city \n",
    "    data['Concert_StartTime'] = data['Concert'].astype(str).str.extract(r'([A-Za-z]+)(\\d+)-', expand=False)[1].astype(float) # Extract end time (the number after the dash, before 'pm')\n",
    "    data['Concert_EndTime'] = data['Concert'].astype(str).str.extract(r'-(\\d+)pm').astype(float)\n",
    "    data['Concert_Duration'] = 12+data['Concert_EndTime'] - data['Concert_StartTime']\n",
    "    data = data.drop(columns=['Concert'])\n",
    "    \n",
    "    # Apply function and create two new columns\n",
    "    data[['Ticket_Type', 'Ticket_Num', 'Ticket_Class']] = data['TicketInfo'].apply(\n",
    "        lambda x: pd.Series(split_ticket(x))\n",
    "    )\n",
    "\n",
    "    # Create dummy variables (One-Hot Encoding)\n",
    "    # drop_first=True avoids multicollinearity (e.g., if not Male, then Female)\n",
    "    still_not_number_cols = ['PreferedAlbum', 'Vinyl', 'VIP', 'Concert_City', 'Ticket_Type', 'Ticket_Class']\n",
    "    df_encoded = pd.get_dummies(data, columns=still_not_number_cols, drop_first=True)\n",
    "    df_final = df_encoded.drop(columns=['Opinion', 'TicketInfo']) # drop processed columns\n",
    "\n",
    "    if save:\n",
    "        df_final.to_csv(filename, index=False)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=preprocess_csv(path+'/train.csv', save=False, filename=path+'/train_cleaned.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = preprocess_csv(path+\"/test.csv\", save=True, filename=path+\"/test_cleaned.csv\")\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pd.DataFrame.corr(data)\n",
    "corr_graph=sns.heatmap(corr, vmin=-1,vmax=1,cmap='viridis')\n",
    "corr_graph.plot()\n",
    "opinion_data = raw_data['Opinion']\n",
    "opinion_data[\"cleanOpinion\"] = raw_data['Opinion'].apply(preprocess_text)\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "opinion_data['sentiment'] = opinion_data['cleanOpinion'].apply(\n",
    "    lambda x: sia.polarity_scores(x)['compound']\n",
    ")\n",
    "opinion_data['FreePass']=raw_data[\"FreePass\"].astype(int)\n",
    "opinion_data['Age']=raw_data[\"Age\"].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra=sns.histplot(data=opinion_data, x='sentiment',y=\"Age\", hue='FreePass', kde=True,element='step',stat='count', bins=33)\n",
    "extra.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['logDrinks']=np.log1p(data['Drinks'])\n",
    "data['logFood']=np.log1p(data['Food'])\n",
    "\n",
    "data_test['logDrinks']=np.log1p(data_test['Drinks'])\n",
    "data_test['logFood']=np.log1p(data_test['Food'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cc938",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['logFoodRate']=data[\"Food\"]/data['AvgTime']\n",
    "data['logDrinkRate']=data[\"logDrinks\"]/data['AvgTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae09ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = sns.histplot(data, x='logFoodRate', hue='FreePass')\n",
    "rates.set_xscale('log')\n",
    "#jnt_fooddrink = sns.JointGrid(data=data, x='Age', y='logFoodRate', hue='FreePass')\n",
    "#jnt_fooddrink.plot(sns.histplot, sns.histplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data -> split training set into (70% / 15% / 15%) #\n",
    "X=data.drop(columns=[\"Id\",\"FreePass\", \"AvgTime\",'Concert_City_NYC', 'Concert_City_SF','Concert_StartTime', 'Concert_EndTime', 'Concert_Duration'])\n",
    "Y=data[\"FreePass\"]\n",
    "IDs=data[\"Id\"]\n",
    "\n",
    "data_test_IDs = data_test[\"Id\"]\n",
    "X_test_ext = data_test.drop(columns=[\"Id\",\"AvgTime\",'Concert_City_NYC', 'Concert_City_SF','Concert_StartTime', 'Concert_EndTime', 'Concert_Duration'])\n",
    "\n",
    "randstate=np.random.randint(0,1000)\n",
    "randstate2=np.random.randint(0,1000)\n",
    "print(f\"{randstate} {randstate2}\")\n",
    "\n",
    "# 100 -> 70/30 #\n",
    "X_train, X_hold, Y_train, Y_hold, ID_train, ID_hold = train_test_split(\n",
    "    X,Y,IDs,\n",
    "    test_size=0.3,\n",
    "    random_state=randstate,\n",
    "    stratify=Y\n",
    ")\n",
    "\n",
    "# 30 -> 15/15 #\n",
    "X_val, X_test, Y_val, Y_test, ID_val, ID_test = train_test_split(\n",
    "    X_hold,Y_hold,ID_hold,\n",
    "    test_size=0.5,\n",
    "    random_state=randstate2,\n",
    "    stratify=Y_hold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LINEAR REGRESSION PIPELINE -- NOT GOOD FOR THIS PROBLEM ##\n",
    "regression_pipeline = Pipeline(\n",
    "    steps=[\n",
    "    (\"scalar\", StandardScaler(with_mean=False)),\n",
    "    (\"regression\", LinearRegression())\n",
    "    ]\n",
    ")\n",
    "# Train #\n",
    "regression_pipeline.fit(X_train,Y_train)\n",
    "\n",
    "# Predict #\n",
    "train_pred = regression_pipeline.predict(X_train)\n",
    "val_pred = regression_pipeline.predict(X_val)\n",
    "test_pred = regression_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "model_output = regression_pipeline.predict(X_test_ext)\n",
    "model_freepass = (model_output >= 0.5)\n",
    "\n",
    "metrics = {\n",
    "    \"train_mse\": mean_squared_error(Y_train, train_pred),\n",
    "    \"val_mse\":   mean_squared_error(Y_val, val_pred),\n",
    "    \"test_mse\":  mean_squared_error(Y_test, test_pred),\n",
    "    \"train_auc\": roc_auc_score(Y_train, train_pred),\n",
    "    \"val_auc\":   roc_auc_score(Y_val, val_pred),\n",
    "    \"test_auc\":  roc_auc_score(Y_test, test_pred),\n",
    "}\n",
    "\n",
    "output = (\n",
    "    data_test_IDs\n",
    "    .to_frame(name=\"Id\")\n",
    "    .assign(FreePass_pred=model_freepass)\n",
    ")\n",
    "#output.to_csv(os.getcwd()+\"/results/linear_regression_jan12.csv\", index=False)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_l1 = Pipeline([\n",
    "    (\"scaler\", RobustScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"liblinear\",\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])\n",
    "dlog_l1 = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"liblinear\",\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])\n",
    "elastic_log = Pipeline([\n",
    "    (\"scaler\", RobustScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        solver=\"saga\",\n",
    "        l1_ratio=0.3,\n",
    "        max_iter=4000,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "grad_boost_tree = GradientBoostingClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.04,\n",
    "    max_depth=3,\n",
    "    random_state=randstate\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=12,\n",
    "    min_samples_leaf=15,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=randstate,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "def train_model(model, x_train, y_train):\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "def eval_model(model, x, y=None, threshold=0.5):\n",
    "    score = model.predict_proba(x)[:,1]\n",
    "    output = score >= threshold\n",
    "    metrics=[]\n",
    "    if y is not None:\n",
    "        metrics.append(roc_auc_score(y,score))\n",
    "        metrics.append(np.sum(y==output)/(len(y)))\n",
    "        metrics.append(log_loss(y, score))\n",
    "    return score, output, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e869844",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [logistic_l1, grad_boost_tree, elastic_log, rf]\n",
    "model_names = [\"logistic_l1\", \"grad_boost_tree\", \"elastic_log\", \"rf\", \"dummy_model\"]\n",
    "models=[]\n",
    "\n",
    "# Train models #\n",
    "for ppl in pipelines:\n",
    "    models.append(train_model(ppl, X_train, Y_train))\n",
    "dummy_answers = Y_train.sample(frac=1.0, random_state=55).values\n",
    "dummy_model = train_model(dlog_l1, X_train, dummy_answers)\n",
    "models.append(dummy_model)\n",
    "\n",
    "thresholds=np.linspace(0.2, 0.8, 40)\n",
    "\n",
    "# Evaluate with validation split\n",
    "metrics_all = []\n",
    "for _,m in enumerate(models):\n",
    "    metrics_model=[]\n",
    "    print(f\"----------- ({str(model_names[_])})\")\n",
    "    for t in thresholds:\n",
    "        scores,output,metrics = eval_model(m, X_test, Y_test, t)\n",
    "        metrics_model.append(metrics)\n",
    "    metrics_all.append(metrics_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics = []\n",
    "for _m, m in enumerate(metrics_all):\n",
    "    acc=[]\n",
    "    roc=[]\n",
    "    logloss=[]\n",
    "    for _t, t in enumerate(metrics_all[_m]):\n",
    "        #print(t)\n",
    "        roc.append(t[0])\n",
    "        acc.append(t[1])\n",
    "        logloss.append(t[2])\n",
    "    roc=np.asarray(roc, dtype=float)\n",
    "    acc=np.asarray(acc, dtype=float)\n",
    "    logloss=np.asarray(logloss, dtype=float)\n",
    "    plot_metrics.append([roc, acc, logloss])\n",
    "plot_metrics=np.asarray(plot_metrics)\n",
    "\n",
    "for _, mdl in enumerate(plot_metrics):\n",
    "    print(f\"{np.max(plot_metrics[_,1,:]):.4f} -- {model_names[_]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_names = [\"roc_auc\", \"acc\", \"logloss\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors=['black', 'blue', 'green', 'red', 'orange']\n",
    "markers = [\".\",\"x\",\"^\"]\n",
    "# Loop over models\n",
    "for i, model in enumerate(plot_metrics):\n",
    "    # Select only this model's rows\n",
    "    \n",
    "    # Plot each metric for this model\n",
    "    for n, mtrc in enumerate(plot_metrics_names):\n",
    "        plt.plot(\n",
    "            thresholds,\n",
    "            plot_metrics[i][n],\n",
    "            marker=markers[n],\n",
    "            color=colors[i],\n",
    "            label=f\"{model_names[i]} - {mtrc}\"\n",
    "        )\n",
    "\n",
    "plt.title(\"Model Performance vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Metric value\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf3301",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Save CSV [NEEDS FIXING -> Modular for each model]\n",
    "\n",
    "output= model_logistic_l1.predict(X_test_ext)\n",
    "output2= model_grad_boost_tree.predict(X_test_ext)\n",
    "output_pred = output >= 0.5\n",
    "output_pred2 = output2 >= 0.5\n",
    "output_df = (\n",
    "    data_test_IDs\n",
    "    .to_frame(name=\"Id\")\n",
    "    .assign(FreePass=output_pred)\n",
    ")\n",
    "output_df\n",
    "\n",
    "output_df.to_csv(os.getcwd()+\"/results/logistic_regression-Alex_Peter-JAN12.csv\", index=False)\n",
    "output_df2 = (\n",
    "    data_test_IDs\n",
    "    .to_frame(name=\"Id\")\n",
    "    .assign(FreePass=output_pred2)\n",
    ")\n",
    "output_df2\n",
    "\n",
    "output_df2.to_csv(os.getcwd()+\"/results/gradient_boosted_tree-Alex_Peter-JAN12.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logl1_coef = pd.Series(logistic_l1.named_steps[\"clf\"].coef_[0], index=X_train.columns)\n",
    "l_sorted=logl1_coef.sort_values(ascending=False)\n",
    "l_sorted\n",
    "\n",
    "gradboosttree_coef = pd.Series(grad_boost_tree.feature_importances_, index=X_train.columns)\n",
    "gbt=gradboosttree_coef.sort_values(ascending=False)\n",
    "gbt\n",
    "\n",
    "leak_check = X_train.assign(FreePass=Y_train)\n",
    "\n",
    "corr = leak_check.corr()[\"FreePass\"].abs().sort_values(ascending=False)\n",
    "corr.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
